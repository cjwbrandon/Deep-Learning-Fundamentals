{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation & Loss Functions\n",
    "\n",
    "Between layers of a Neural Network, we pass values derived from the previous layer. We call this set of values the Activations of the layer. Before we pass the values, we would apply some form of Activation Function to introduce non-linearity to the Network.\n",
    "\n",
    "I will be consolidating all types of Activation Functions I know of in this notebook.\n",
    "List of Activation Functions:\n",
    "1. Sigmoid\n",
    "    - 1 / (1+e^-x)\n",
    "    - 0 to 1\n",
    "2. Tanh - -1 to 1 \n",
    "    - (e^z - e^-z) / (e^z + e ^-z)\n",
    "    - -1 to 1\n",
    "3. ReLU\n",
    "4. Leaky ReLU\n",
    "5. Softmax\n",
    "\n",
    "List of Loss Functions:\n",
    "1. Cross-Entropy\n",
    "    - −(ylog(p)+(1−y)log(1−p))\n",
    "    - Used for Binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid + Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_classification(n_samples=1000, \n",
    "                           n_features=2, \n",
    "                           n_classes=2,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=0,\n",
    "                           flip_y=0.1,\n",
    "                           class_sep=0.9\n",
    "                          )\n",
    "\n",
    "x = torch.from_numpy(x).float()\n",
    "y = torch.from_numpy(y).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "sigmoid_model = nn.Sequential(\n",
    "    nn.Linear(2,2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# loss function - CrossEntropy, optimiser - SGD\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(sigmoid_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000 --- Loss: 0.6427337527275085\n",
      "11/1000 --- Loss: 0.6298125386238098\n",
      "21/1000 --- Loss: 0.6181340217590332\n",
      "31/1000 --- Loss: 0.6075857877731323\n",
      "41/1000 --- Loss: 0.5980523228645325\n",
      "51/1000 --- Loss: 0.5894248485565186\n",
      "61/1000 --- Loss: 0.5816006064414978\n",
      "71/1000 --- Loss: 0.5744912028312683\n",
      "81/1000 --- Loss: 0.5680191516876221\n",
      "91/1000 --- Loss: 0.5621114373207092\n",
      "101/1000 --- Loss: 0.556709349155426\n",
      "111/1000 --- Loss: 0.5517580509185791\n",
      "121/1000 --- Loss: 0.5472099781036377\n",
      "131/1000 --- Loss: 0.5430224537849426\n",
      "141/1000 --- Loss: 0.5391596555709839\n",
      "151/1000 --- Loss: 0.5355876088142395\n",
      "161/1000 --- Loss: 0.5322769284248352\n",
      "171/1000 --- Loss: 0.5292025804519653\n",
      "181/1000 --- Loss: 0.5263410806655884\n",
      "191/1000 --- Loss: 0.5236734747886658\n",
      "201/1000 --- Loss: 0.5211797952651978\n",
      "211/1000 --- Loss: 0.51884526014328\n",
      "221/1000 --- Loss: 0.5166550874710083\n",
      "231/1000 --- Loss: 0.5145966410636902\n",
      "241/1000 --- Loss: 0.5126585960388184\n",
      "251/1000 --- Loss: 0.5108311176300049\n",
      "261/1000 --- Loss: 0.5091041922569275\n",
      "271/1000 --- Loss: 0.5074710249900818\n",
      "281/1000 --- Loss: 0.5059223175048828\n",
      "291/1000 --- Loss: 0.5044536590576172\n",
      "301/1000 --- Loss: 0.5030576586723328\n",
      "311/1000 --- Loss: 0.5017288327217102\n",
      "321/1000 --- Loss: 0.50046306848526\n",
      "331/1000 --- Loss: 0.4992557168006897\n",
      "341/1000 --- Loss: 0.4981023371219635\n",
      "351/1000 --- Loss: 0.4969989061355591\n",
      "361/1000 --- Loss: 0.49594324827194214\n",
      "371/1000 --- Loss: 0.49493175745010376\n",
      "381/1000 --- Loss: 0.4939616322517395\n",
      "391/1000 --- Loss: 0.49302971363067627\n",
      "401/1000 --- Loss: 0.49213430285453796\n",
      "411/1000 --- Loss: 0.49127307534217834\n",
      "421/1000 --- Loss: 0.49044403433799744\n",
      "431/1000 --- Loss: 0.4896455705165863\n",
      "441/1000 --- Loss: 0.48887550830841064\n",
      "451/1000 --- Loss: 0.4881322383880615\n",
      "461/1000 --- Loss: 0.48741450905799866\n",
      "471/1000 --- Loss: 0.4867212176322937\n",
      "481/1000 --- Loss: 0.4860509932041168\n",
      "491/1000 --- Loss: 0.4854021966457367\n",
      "501/1000 --- Loss: 0.48477357625961304\n",
      "511/1000 --- Loss: 0.48416513204574585\n",
      "521/1000 --- Loss: 0.4835750460624695\n",
      "531/1000 --- Loss: 0.4830033481121063\n",
      "541/1000 --- Loss: 0.48244810104370117\n",
      "551/1000 --- Loss: 0.4819090664386749\n",
      "561/1000 --- Loss: 0.48138538002967834\n",
      "571/1000 --- Loss: 0.48087644577026367\n",
      "581/1000 --- Loss: 0.48038169741630554\n",
      "591/1000 --- Loss: 0.4799003601074219\n",
      "601/1000 --- Loss: 0.47943174839019775\n",
      "611/1000 --- Loss: 0.47897544503211975\n",
      "621/1000 --- Loss: 0.4785313904285431\n",
      "631/1000 --- Loss: 0.4780987799167633\n",
      "641/1000 --- Loss: 0.4776766896247864\n",
      "651/1000 --- Loss: 0.4772648513317108\n",
      "661/1000 --- Loss: 0.47686290740966797\n",
      "671/1000 --- Loss: 0.4764713644981384\n",
      "681/1000 --- Loss: 0.4760882556438446\n",
      "691/1000 --- Loss: 0.4757150709629059\n",
      "701/1000 --- Loss: 0.47535014152526855\n",
      "711/1000 --- Loss: 0.47499385476112366\n",
      "721/1000 --- Loss: 0.47464489936828613\n",
      "731/1000 --- Loss: 0.47430381178855896\n",
      "741/1000 --- Loss: 0.4739700257778168\n",
      "751/1000 --- Loss: 0.4736438989639282\n",
      "761/1000 --- Loss: 0.4733242094516754\n",
      "771/1000 --- Loss: 0.4730120897293091\n",
      "781/1000 --- Loss: 0.47270581126213074\n",
      "791/1000 --- Loss: 0.4724060297012329\n",
      "801/1000 --- Loss: 0.4721120297908783\n",
      "811/1000 --- Loss: 0.4718243479728699\n",
      "821/1000 --- Loss: 0.4715420603752136\n",
      "831/1000 --- Loss: 0.47126519680023193\n",
      "841/1000 --- Loss: 0.4709937572479248\n",
      "851/1000 --- Loss: 0.47072750329971313\n",
      "861/1000 --- Loss: 0.4704667627811432\n",
      "871/1000 --- Loss: 0.4702105224132538\n",
      "881/1000 --- Loss: 0.46995845437049866\n",
      "891/1000 --- Loss: 0.4697120189666748\n",
      "901/1000 --- Loss: 0.4694693982601166\n",
      "911/1000 --- Loss: 0.4692314565181732\n",
      "921/1000 --- Loss: 0.4689975380897522\n",
      "931/1000 --- Loss: 0.4687670171260834\n",
      "941/1000 --- Loss: 0.46854159235954285\n",
      "951/1000 --- Loss: 0.46832001209259033\n",
      "961/1000 --- Loss: 0.46810072660446167\n",
      "971/1000 --- Loss: 0.46788641810417175\n",
      "981/1000 --- Loss: 0.4676757752895355\n",
      "991/1000 --- Loss: 0.46746826171875\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 1000\n",
    "loss = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    optimiser.zero_grad() # zero gradients\n",
    "    \n",
    "    output = sigmoid_model.forward(x) # forward prop\n",
    "    running_loss = criterion(output, y) # calculate loss\n",
    "    loss.append(running_loss) # store loss\n",
    "    running_loss.backward() # back prop\n",
    "    optimiser.step() #  update weights\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print('{}/{} --- Loss: {}'.format(e+1, epochs, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9999, grad_fn=<MaxBackward1>)\n",
      "tensor(8.3125e-05, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(output.max())\n",
    "print(output.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our values through the sigmoid function, we can see that they are restricted between 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3863, 0.5450],\n",
       "        [0.0499, 0.9405],\n",
       "        [0.9589, 0.0344],\n",
       "        [0.4205, 0.5515],\n",
       "        [0.5080, 0.4148]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be interpretted as the probability of the class. i.e. column 1 --> probability of class 0 and column 2 --> probability of class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take larger probability as the predicted class\n",
    "_, predictions = torch.max(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87       503\n",
      "           1       0.84      0.92      0.88       497\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.88      0.87      0.87      1000\n",
      "weighted avg       0.88      0.87      0.87      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View classical classification scoring methods\n",
    "print(classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "tanh_model = nn.Sequential(\n",
    "    nn.Linear(2,2),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "# loss function - CrossEntropy, optimiser - SGD\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimiser = optim.SGD(tanh_model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1000 --- Loss: 0.9230818748474121\n",
      "11/1000 --- Loss: 0.7356969118118286\n",
      "21/1000 --- Loss: 0.6006231307983398\n",
      "31/1000 --- Loss: 0.5193090438842773\n",
      "41/1000 --- Loss: 0.4716643989086151\n",
      "51/1000 --- Loss: 0.44304099678993225\n",
      "61/1000 --- Loss: 0.4247363209724426\n",
      "71/1000 --- Loss: 0.41223159432411194\n",
      "81/1000 --- Loss: 0.4032110571861267\n",
      "91/1000 --- Loss: 0.3964187800884247\n",
      "101/1000 --- Loss: 0.39113208651542664\n",
      "111/1000 --- Loss: 0.3869064748287201\n",
      "121/1000 --- Loss: 0.38345783948898315\n",
      "131/1000 --- Loss: 0.3805941343307495\n",
      "141/1000 --- Loss: 0.37818092107772827\n",
      "151/1000 --- Loss: 0.37612324953079224\n",
      "161/1000 --- Loss: 0.374348908662796\n",
      "171/1000 --- Loss: 0.3728054165840149\n",
      "181/1000 --- Loss: 0.3714505136013031\n",
      "191/1000 --- Loss: 0.3702529966831207\n",
      "201/1000 --- Loss: 0.36918750405311584\n",
      "211/1000 --- Loss: 0.3682333827018738\n",
      "221/1000 --- Loss: 0.36737409234046936\n",
      "231/1000 --- Loss: 0.3665965795516968\n",
      "241/1000 --- Loss: 0.36589014530181885\n",
      "251/1000 --- Loss: 0.36524486541748047\n",
      "261/1000 --- Loss: 0.3646537959575653\n",
      "271/1000 --- Loss: 0.3641100227832794\n",
      "281/1000 --- Loss: 0.3636082708835602\n",
      "291/1000 --- Loss: 0.3631432354450226\n",
      "301/1000 --- Loss: 0.3627115786075592\n",
      "311/1000 --- Loss: 0.3623097538948059\n",
      "321/1000 --- Loss: 0.3619346022605896\n",
      "331/1000 --- Loss: 0.3615837097167969\n",
      "341/1000 --- Loss: 0.3612544536590576\n",
      "351/1000 --- Loss: 0.36094531416893005\n",
      "361/1000 --- Loss: 0.3606535792350769\n",
      "371/1000 --- Loss: 0.3603789806365967\n",
      "381/1000 --- Loss: 0.3601193130016327\n",
      "391/1000 --- Loss: 0.35987380146980286\n",
      "401/1000 --- Loss: 0.3596404790878296\n",
      "411/1000 --- Loss: 0.35941892862319946\n",
      "421/1000 --- Loss: 0.35920849442481995\n",
      "431/1000 --- Loss: 0.35900846123695374\n",
      "441/1000 --- Loss: 0.35881760716438293\n",
      "451/1000 --- Loss: 0.35863494873046875\n",
      "461/1000 --- Loss: 0.3584606647491455\n",
      "471/1000 --- Loss: 0.3582937717437744\n",
      "481/1000 --- Loss: 0.35813406109809875\n",
      "491/1000 --- Loss: 0.35798075795173645\n",
      "501/1000 --- Loss: 0.3578338921070099\n",
      "511/1000 --- Loss: 0.3576927185058594\n",
      "521/1000 --- Loss: 0.35755693912506104\n",
      "531/1000 --- Loss: 0.35742655396461487\n",
      "541/1000 --- Loss: 0.35730069875717163\n",
      "551/1000 --- Loss: 0.3571793735027313\n",
      "561/1000 --- Loss: 0.35706251859664917\n",
      "571/1000 --- Loss: 0.3569493293762207\n",
      "581/1000 --- Loss: 0.35684025287628174\n",
      "591/1000 --- Loss: 0.35673442482948303\n",
      "601/1000 --- Loss: 0.35663217306137085\n",
      "611/1000 --- Loss: 0.3565334975719452\n",
      "621/1000 --- Loss: 0.3564375638961792\n",
      "631/1000 --- Loss: 0.3563444912433624\n",
      "641/1000 --- Loss: 0.3562541902065277\n",
      "651/1000 --- Loss: 0.3561665415763855\n",
      "661/1000 --- Loss: 0.356081485748291\n",
      "671/1000 --- Loss: 0.35599881410598755\n",
      "681/1000 --- Loss: 0.3559185862541199\n",
      "691/1000 --- Loss: 0.35583993792533875\n",
      "701/1000 --- Loss: 0.355763703584671\n",
      "711/1000 --- Loss: 0.35569003224372864\n",
      "721/1000 --- Loss: 0.3556174039840698\n",
      "731/1000 --- Loss: 0.35554689168930054\n",
      "741/1000 --- Loss: 0.3554776608943939\n",
      "751/1000 --- Loss: 0.35541030764579773\n",
      "761/1000 --- Loss: 0.35534489154815674\n",
      "771/1000 --- Loss: 0.35528048872947693\n",
      "781/1000 --- Loss: 0.35521769523620605\n",
      "791/1000 --- Loss: 0.3551563024520874\n",
      "801/1000 --- Loss: 0.35509610176086426\n",
      "811/1000 --- Loss: 0.35503721237182617\n",
      "821/1000 --- Loss: 0.35497957468032837\n",
      "831/1000 --- Loss: 0.3549230694770813\n",
      "841/1000 --- Loss: 0.35486748814582825\n",
      "851/1000 --- Loss: 0.35481318831443787\n",
      "861/1000 --- Loss: 0.3547596335411072\n",
      "871/1000 --- Loss: 0.3547073304653168\n",
      "881/1000 --- Loss: 0.354655921459198\n",
      "891/1000 --- Loss: 0.3546052575111389\n",
      "901/1000 --- Loss: 0.3545559048652649\n",
      "911/1000 --- Loss: 0.3545069694519043\n",
      "921/1000 --- Loss: 0.3544590175151825\n",
      "931/1000 --- Loss: 0.35441187024116516\n",
      "941/1000 --- Loss: 0.35436534881591797\n",
      "951/1000 --- Loss: 0.3543192148208618\n",
      "961/1000 --- Loss: 0.35427406430244446\n",
      "971/1000 --- Loss: 0.35422974824905396\n",
      "981/1000 --- Loss: 0.35418587923049927\n",
      "991/1000 --- Loss: 0.3541427254676819\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 1000\n",
    "loss = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    optimiser.zero_grad() # zero gradients\n",
    "    \n",
    "    output = tanh_model.forward(x) # forward prop\n",
    "    running_loss = criterion(output, y) # calculate loss\n",
    "    loss.append(running_loss) # store loss\n",
    "    running_loss.backward() # back prop\n",
    "    optimiser.step() #  update weights\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print('{}/{} --- Loss: {}'.format(e+1, epochs, running_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just off the loss values from training, we can see that using the Tanh function, our loss converges to a much smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, grad_fn=<MaxBackward1>)\n",
      "tensor(-1.0000, grad_fn=<MinBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(output.max())\n",
    "print(output.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Sigmoid function, the Tanh is restricted between -1 to 1 instead. As such, it cannot be interpreted as a probability of the class. The improved learning is often attributed to the Tanh function being centered around 0 unlike the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take larger probability as the predicted class\n",
    "_, predictions = torch.max(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.84      0.87       503\n",
      "           1       0.85      0.91      0.88       497\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View classical classification scoring methods\n",
    "print(classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
