{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this series of notebooks, I intend to experiment with how Neural Networks fundamentally works as well as different environments affects how the Neural Network performs.\n",
    "\n",
    "I would also like to reference to Andrew Ng's Deep Learning Specialisation on Coursera. I will be using these experiments as a form of practicing the theories that were presented in the courses and thus, will be largely aligned with the course itself. You will be able to find the course via this link: https://www.coursera.org/specializations/deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import distributions as dist\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN & Linear Regression Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random data - Linear Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform distribution from 0 to 100\n",
    "x = torch.rand(100, 2) * 100\n",
    "# y = 0.8.x1 + 1.5.x2 + noise\n",
    "y = (0.8*x[:, 0] + 1.5*x[:, 1]).resize_((100, 1))\n",
    "noise = dist.normal.Normal(torch.tensor([0.0]), torch.tensor([2.0]))\n",
    "y += noise.sample(torch.Size([100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.785756 , 1.5100229]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef = lin_reg.coef_\n",
    "coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.455973"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_loss = mean_squared_error(y, lin_reg.predict(x))\n",
    "lin_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-unit NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 neuron taking in 2 variables as inputs\n",
    "one_model = nn.Sequential(\n",
    "    nn.Linear(2,1)\n",
    ")\n",
    "\n",
    "# loss function - MSE, optimiser - SGD\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(one_model.parameters(), lr=0.0001) # Define our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since NN is trained using Gradient Descent, we need to create a loop to train our model\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad() # zero out gradients before we train\n",
    "    \n",
    "    output = one_model.forward(x) # forward propagation\n",
    "    loss  = criterion(output, y) # calculate cost function\n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # 1 step in updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7825, 1.5072]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(one_model.parameters())[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4748077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, one_model.forward(x).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since NN is trained using Gradient Descent, we need to create a loop to train our model\n",
    "epochs = 1000\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad() # zero out gradients before we train\n",
    "    \n",
    "    output = one_model.forward(x) # forward propagation\n",
    "    loss  = criterion(output, y) # calculate cost function\n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # 1 step in updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7826, 1.5073]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(one_model.parameters())[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4738483"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, one_model.forward(x).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since NN is trained using Gradient Descent, we need to create a loop to train our model\n",
    "epochs = 100000\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad() # zero out gradients before we train\n",
    "    \n",
    "    output = one_model.forward(x) # forward propagation\n",
    "    loss  = criterion(output, y) # calculate cost function\n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # 1 step in updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.785524 , 1.5098207], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_coef = list(one_model.parameters())[0].data.numpy()[0]\n",
    "nn_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4560676"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_loss = mean_squared_error(y, one_model.forward(x).data.numpy())\n",
    "nn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that as our training loops approaches infinity, our trained coefficients essentially approachs that of the Linear Regression. Fundamentally, a 1 neuron NN is essentially a Linear Regression model that is optimised using Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How NN or ML in general strives off data\n",
    "\n",
    "As seen above, both the NN and the Linear Regression models were unable to converge toward the exact weights used to generate the dataset. I will be exploring how having more data can actually solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the samples to 10,000\n",
    "sample_size = 10000\n",
    "# Uniform distribution from 0 to 100\n",
    "x = torch.rand(sample_size, 2) * 100\n",
    "# y = 0.8.x1 + 1.5.x2 + noise\n",
    "y = (0.8*x[:, 0] + 1.5*x[:, 1]).resize_((sample_size, 1))\n",
    "noise = dist.normal.Normal(torch.tensor([0.0]), torch.tensor([2.0]))\n",
    "y += noise.sample(torch.Size([sample_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01424401, 0.01002288]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Reg: difference between old coefficients and actual coefficients\n",
    "abs([0.8, 1.5] - coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.455973"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old MSE\n",
    "lin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x, y)\n",
    "coef = lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.98635864e-05, 5.26189804e-04]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Reg: difference between new coefficients and actual coefficients\n",
    "abs([0.8, 1.5] - coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0370216"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_loss = mean_squared_error(y, lin_reg.predict(x))\n",
    "lin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  retraining the NN\n",
    "# 1 neuron taking in 2 variables as inputs\n",
    "one_model = nn.Sequential(\n",
    "    nn.Linear(2,1)\n",
    ")\n",
    "\n",
    "# loss function - MSE, optimiser - SGD\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(one_model.parameters(), lr=0.0001) # Define our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01447599, 0.0098207 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-unit NN: difference between old coefficients and actual coefficients\n",
    "abs([0.8, 1.5] - nn_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since NN is trained using Gradient Descent, we need to create a loop to train our model\n",
    "epochs = 100000\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad() # zero out gradients before we train\n",
    "    \n",
    "    output = one_model.forward(x) # forward propagation\n",
    "    loss  = criterion(output, y) # calculate cost function\n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # 1 step in updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7999441, 1.5005094], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_coef = list(one_model.parameters())[0].data.numpy()[0]\n",
    "nn_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.58972359e-05, 5.09381294e-04])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-unit NN: difference between new coefficients and actual coefficients\n",
    "abs([0.8, 1.5] - nn_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.037021"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_loss = mean_squared_error(y, one_model.forward(x).data.numpy())\n",
    "nn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the coefficients gets closer to the actual values despite the noise. This is largely because with more data points, we are able to cover the distribution space better and thus, our models are able to better map the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gradient Descent? (simplified)\n",
    "\n",
    "At the start, a NN has to start its weights (coefficients) somewhere. We call this the random initialisation of weights where there are a bunch of techniques that we will cover later. Due to this random 'starting point', it is unlikely for the NN to do well at the start. This would lead to very inaccurate predictions that we present using our loss/ cost function. As such, in order to do better at the predictions, we need the NN to learn as it observes these data points and this is done using Gradient Descent.\n",
    "\n",
    "How this happens is through a multiple step process:\n",
    "1. Forward propagation --> we pass the data points through the Network to calculate a prediction\n",
    "2. Calculate Cost function --> How accurate did the Network do this time?\n",
    "3. Back propagtion --> Using our errors, we can improve our network by tweaking the parameters (coefficients). Back propagation calculates by how much should we tweak the parameters.\n",
    "4. Update Weights (step) --> We update the parameters (coefficients) using the calculated values and restart the cycle.\n",
    "\n",
    "Through this process, we can consistently make better and better predictions as the Network continues to learn from its error and slowly descent down to the optimal solution using the gradients calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  retraining the NN\n",
    "# 1 neuron taking in 2 variables as inputs\n",
    "one_model = nn.Sequential(\n",
    "    nn.Linear(2,1)\n",
    ")\n",
    "\n",
    "# loss function - MSE, optimiser - SGD\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(one_model.parameters(), lr=0.0001) # Define our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 28298.03125\n",
      "Training loss: 624.4863891601562\n",
      "Training loss: 27.414464950561523\n",
      "Training loss: 11.572626113891602\n",
      "Training loss: 9.108637809753418\n",
      "Training loss: 7.57782506942749\n",
      "Training loss: 6.516835689544678\n",
      "Training loss: 5.778843402862549\n",
      "Training loss: 5.26546573638916\n",
      "Training loss: 4.908335208892822\n",
      "Training loss: 4.6598944664001465\n",
      "Training loss: 4.487067222595215\n",
      "Training loss: 4.3668389320373535\n",
      "Training loss: 4.283201217651367\n",
      "Training loss: 4.22501802444458\n",
      "Training loss: 4.18454122543335\n",
      "Training loss: 4.156383514404297\n",
      "Training loss: 4.136794567108154\n",
      "Training loss: 4.123166084289551\n",
      "Training loss: 4.113684177398682\n",
      "Training loss: 4.107088565826416\n",
      "Training loss: 4.102499008178711\n",
      "Training loss: 4.099304676055908\n",
      "Training loss: 4.097081661224365\n",
      "Training loss: 4.095536231994629\n",
      "Training loss: 4.094457626342773\n",
      "Training loss: 4.0937066078186035\n",
      "Training loss: 4.093184947967529\n",
      "Training loss: 4.0928192138671875\n",
      "Training loss: 4.092565059661865\n",
      "Training loss: 4.092386722564697\n",
      "Training loss: 4.09226131439209\n",
      "Training loss: 4.0921735763549805\n",
      "Training loss: 4.092113971710205\n",
      "Training loss: 4.092069149017334\n",
      "Training loss: 4.092037677764893\n",
      "Training loss: 4.092013835906982\n",
      "Training loss: 4.091996669769287\n",
      "Training loss: 4.091983795166016\n",
      "Training loss: 4.09197473526001\n",
      "Training loss: 4.09196662902832\n",
      "Training loss: 4.0919599533081055\n",
      "Training loss: 4.091955661773682\n",
      "Training loss: 4.0919508934021\n",
      "Training loss: 4.091946601867676\n",
      "Training loss: 4.091941833496094\n",
      "Training loss: 4.091939926147461\n",
      "Training loss: 4.091935634613037\n",
      "Training loss: 4.09193229675293\n",
      "Training loss: 4.091929912567139\n",
      "Training loss: 4.091926097869873\n",
      "Training loss: 4.091921806335449\n",
      "Training loss: 4.091919898986816\n",
      "Training loss: 4.091917037963867\n",
      "Training loss: 4.091912746429443\n",
      "Training loss: 4.091909885406494\n",
      "Training loss: 4.091907024383545\n",
      "Training loss: 4.091901779174805\n",
      "Training loss: 4.09190034866333\n",
      "Training loss: 4.091897964477539\n",
      "Training loss: 4.091893672943115\n",
      "Training loss: 4.091890811920166\n",
      "Training loss: 4.091888427734375\n",
      "Training loss: 4.091883659362793\n",
      "Training loss: 4.091880798339844\n",
      "Training loss: 4.091879367828369\n",
      "Training loss: 4.0918755531311035\n",
      "Training loss: 4.091871738433838\n",
      "Training loss: 4.091869354248047\n",
      "Training loss: 4.0918660163879395\n",
      "Training loss: 4.091862678527832\n",
      "Training loss: 4.091860771179199\n",
      "Training loss: 4.091857433319092\n",
      "Training loss: 4.091854095458984\n",
      "Training loss: 4.091850757598877\n",
      "Training loss: 4.091846942901611\n",
      "Training loss: 4.091843605041504\n",
      "Training loss: 4.0918402671813965\n",
      "Training loss: 4.09183931350708\n",
      "Training loss: 4.091835021972656\n",
      "Training loss: 4.091831207275391\n",
      "Training loss: 4.0918288230896\n",
      "Training loss: 4.09182596206665\n",
      "Training loss: 4.091821670532227\n",
      "Training loss: 4.091819763183594\n",
      "Training loss: 4.0918169021606445\n",
      "Training loss: 4.091813087463379\n",
      "Training loss: 4.09181022644043\n",
      "Training loss: 4.0918073654174805\n",
      "Training loss: 4.091803073883057\n",
      "Training loss: 4.091800212860107\n",
      "Training loss: 4.091799259185791\n",
      "Training loss: 4.091795444488525\n",
      "Training loss: 4.09179162979126\n",
      "Training loss: 4.0917887687683105\n",
      "Training loss: 4.091785430908203\n",
      "Training loss: 4.091780662536621\n",
      "Training loss: 4.09177827835083\n",
      "Training loss: 4.0917768478393555\n",
      "Training loss: 4.091773509979248\n"
     ]
    }
   ],
   "source": [
    "# Reusing the training loop above\n",
    "epochs = 100\n",
    "running_loss = [] # store our loss after each step\n",
    "\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad() # zero out gradients before we train\n",
    "    \n",
    "    output = one_model.forward(x) # forward propagation\n",
    "    loss  = criterion(output, y) # calculate cost function\n",
    "    running_loss.append(loss) # store loss\n",
    "    \n",
    "    loss.backward() # calculate gradients\n",
    "    optimizer.step() # 1 step in updating weights\n",
    "    print(f\"Training loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96e3a9f9d0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJAAAAI/CAYAAAAoSiMoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdW4yd530e+ued4ZAjUjyJM2tZJ1uWRHGW7cSOo9iyJTvSrInj5MYp0Iv0ovFFAAOtA/R00fTKRYsCaYG2QLDbAElj1AGKpkHbjRjYbrw9Q8qysWNZlKM4SWdIStSBsh3OkEPKPIiHmfn2BZdSWqKGp5n51uH3AxbWzDvft/gsQDd68L3/t1RVFQAAAAB4N0N1BwAAAACguymQAAAAAFiVAgkAAACAVSmQAAAAAFiVAgkAAACAVSmQAAAAAFjVproD3KqxsbHqgQceqDsGAAAAQN94/vnnT1RVNf729Z4tkB544IEcPHiw7hgAAAAAfaOU8uq11m1hAwAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCiQAAAAAVqVAAgAAAGBVCqSaVVVVdwQAAACAVSmQanRgbj6f/K39mT9zoe4oAAAAAO9KgVSj5o7R/OiNCzkwN193FAAAAIB3pUCqUevu7bln52imZxVIAAAAQPdSINWolJJ2q5lvHzmRC5eX644DAAAAcE0KpJpNthp58/Jy/vToybqjAAAAAFyTAqlmn3hwT7ZuHs7M7PG6owAAAABckwKpZqMjw3ni4bHsn51PVVV1xwEAAAB4BwVSF5hqNfPDNy5k9kdn6o4CAAAA8A4KpC7w5MR4ktjGBgAAAHQlBVIXaGwfzYfv35Xpufm6owAAAAC8gwKpS0xNNPLnx05n/syFuqMAAAAA/AQFUpdot5pJkqfnFmpOAgAAAPCTFEhdonX39tyzczTT5iABAAAAXUaB1CVKKZlsNfKtIydy4fJy3XEAAAAA/oYCqYu0W828eXk5f3r0ZN1RAAAAAP6GAqmLfOLBPbljZDj7Z53GBgAAAHQPBVIXGR0ZzhN7xzIzezxVVdUdBwAAACCJAqnrTLUa+eEbFzL7ozN1RwEAAABIokDqOk9NNJIkM05jAwAAALqEAqnLNLaP5sP378r0nDlIAAAAQHdQIHWhqYlG/vzY6SycuVh3FAAAAAAFUjeabF3ZxnbAU0gAAABAF1AgdaEP3L0j9+wczbQ5SAAAAEAXUCB1oVJKJluNfOvIiVy4vFx3HAAAAGDAKZC6VLvVzJuXl/OdoyfrjgIAAAAMOAVSl/rEg3tyx8hwZmbNQQIAAADqpUDqUqMjw3li71hmZo+nqqq64wAAAAADTIHUxaZajfzwjQuZ/dGZuqMAAAAAA0yB1MWemmgkSfbPOY0NAAAAqI8CqYs1to/mw/ftzLQ5SAAAAECNFEhdrt1q5s9fP52FMxfrjgIAAAAMKAVSl2u3Gqmq5MCcp5AAAACAeiiQutwH7t6Ru3eOZsYcJAAAAKAmCqQuV0rJ5EQj3zpyIhcuL9cdBwAAABhACqQeMNVq5vyl5Xzn6Mm6owAAAAADSIHUAz7x0J7cMTKcGaexAQAAADVQIPWA0ZHhPLF3LPvn5lNVVd1xAAAAgAGjQOoR7YlGfnD6zcz99Zm6owAAAAADRoHUIyYnGkmSmVmnsQEAAAAbS4HUIxo7RvPh+3Zm2hwkAAAAYIMpkHpIu9XMn79+OgtnLtYdBQAAABggCqQeMjnRSFUlBw55CgkAAADYOAqkHvLBe3bk7p2j5iABAAAAG0qB1ENKKZmcaORbR07kwuXluuMAAAAAA0KB1GOmWs2cv7Sc7xw9WXcUAAAAYEAokHrMJx7akztGhrN/zhwkAAAAYGMokHrM6MhwHn94LDOz86mqqu44AAAAwABQIPWgqVYjPzj9Zub++kzdUQAAAIABoEDqQZMTjSRxGhsAAACwIRRIPaixYzQfvm9nZsxBAgAAADaAAqlHTU4088Kx0zlx9mLdUQAAAIA+p0DqUe1WI1UVp7EBAAAA606B1KM+eM+O3L1z1BwkAAAAYN0pkHpUKSWTE41868iJXFxarjsOAAAA0McUSD2s3Wrk/KXlfOfoYt1RAAAAgD523QKplHJ/KeVAKWW2lPJXpZR/0Fn/56WUH5RSXui8fvmqe/5ZKeXFUsqhUsovXrX+2c7ai6WU37xq/f2llGdLKUdKKf+tlLJ5rb9oP/rkQ2MZHRmyjQ0AAABYVzfyBNJSkn9SVVUryWNJvlhK+UDnb/++qqqPdF5fS5LO3341yQeTfDbJfyylDJdShpP8hyS/lOQDSf7OVZ/zrzuftTfJqSS/vkbfr6+NjgzniYfHMzM7n6qq6o4DAAAA9KnrFkhVVf2oqqrvdX4+k2Q2yb2r3PK5JH9YVdXFqqpeTvJiko91Xi9WVXW0qqpLSf4wyedKKSXJZJL/3rn/K0l+5Va/0KCZajXyg9Nv5tDxM3VHAQAAAPrUTc1AKqU8kORnkjzbWfqNUsr3SylfLqXs7qzdm+TYVbe93ll7t/U9SU5XVbX0tnVuwOREI0kyMztfcxIAAACgX91wgVRKuTPJ/0jyD6uq+nGS30nyUJKPJPlRkn/71qXXuL26hfVrZfhCKeVgKeXgwsLCjUbva40do/np+3Zm2hwkAAAAYJ3cUIFUShnJlfLov1RV9T+TpKqq41VVLVdVtZLk93Jli1py5Qmi+6+6/b4kP1xl/USSXaWUTW9bf4eqqn63qqpHq6p6dHx8/EaiD4T2RDMvHDudE2cv1h0FAAAA6EM3cgpbSfL7SWarqvp3V63ffdVlfyvJX3Z+/mqSXy2lbCmlvD/J3iTfTfJckr2dE9c258qg7a9WV6Y/H0jytzv3fz7JH9/e1xos7VYjVZUcmLONDQAAAFh7N/IE0uNJ/m6SyVLKC53XLyf5N6WUvyilfD/JU0n+UZJUVfVXSf4oyf9O8idJvth5UmkpyW8k+XquDOL+o861SfJPk/zjUsqLuTIT6ffX7iv2vw/esyPv2TFqDhIAAACwLjZd74Kqqr6da88p+toq9/yrJP/qGutfu9Z9VVUdzf/ZAsdNKqVkstXIH//ZD3JxaTlbNg3XHQkAAADoIzd1Chvda6rVyLlLy/nO0cW6owAAAAB9RoHUJz750FhGR4ay32lsAAAAwBpTIPWJ0ZHhPPHwWKZn53NlLjkAAADA2lAg9ZF2q5kfnH4zh46fqTsKAAAA0EcUSH2kPdFIEqexAQAAAGtKgdRHGjtG89P37cy0OUgAAADAGlIg9Zn2RDMvHDudE2cv1h0FAAAA6BMKpD7TbjVSVcmBOdvYAAAAgLWhQOozH7xnR96zY9QcJAAAAGDNKJD6TCklk61GvnVkIReXluuOAwAAAPQBBVIfmmo1cu7Scp49ulh3FAAAAKAPKJD60CcfGsvoyFBmnMYGAAAArAEFUh8aHRnOEw+PZXp2PlVV1R0HAAAA6HEKpD7VbjXzg9Nv5tDxM3VHAQAAAHqcAqlPTU40ksRpbAAAAMBtUyD1qeaO0fzUvTvNQQIAAABumwKpj7VbjfzZsdM5cfZi3VEAAACAHqZA6mNTrWaqKjkwZxsbAAAAcOsUSH3sg/fsSHPHluxXIAEAAAC3QYHUx0opmZxo5pnDC7m4tFx3HAAAAKBHKZD63FSrkXOXlvPs0cW6owAAAAA9SoHU5x5/eCyjI0NOYwMAAABumQKpz42ODOeJh8cyMzefqqrqjgMAAAD0IAXSAJicaOb1U2/m8PGzdUcBAAAAepACaQC0W40kybRtbAAAAMAtUCANgOaO0fzUvTvNQQIAAABuiQJpQLRbjfzZsdM5efZi3VEAAACAHqNAGhDtiWaqKjlwaKHuKAAAAECPUSANiA/duyPNHVtsYwMAAABumgJpQJRSMjnRzDOHF3JxabnuOAAAAEAPUSANkKlWI+cuLefZo4t1RwEAAAB6iAJpgDz+8FhGR4ayf26+7igAAABAD1EgDZDRkeE8/tBYpmePp6qquuMAAAAAPUKBNGDarWZeP/VmDh8/W3cUAAAAoEcokAZMu9VIkkw7jQ0AAAC4QQqkAdPcMZqfunenOUgAAADADVMgDaDJiUa+99qpnDx7se4oAAAAQA9QIA2gqVYzVZUcOLRQdxQAAACgByiQBtCH7t2R5o4tmTEHCQAAALgBCqQBVErJ5EQzzxxeyKWllbrjAAAAAF1OgTSg2hONnLu0nGdfPll3FAAAAKDLKZAG1OMPj2XLpqHMzDqNDQAAAFidAmlA3bF5OE88PJbp2eOpqqruOAAAAEAXUyANsHarmddPvZkj82frjgIAAAB0MQXSAJucaCRJpp3GBgAAAKxCgTTA3rNzNB+6d4c5SAAAAMCqFEgDrj3RzPdeO5WTZy/WHQUAAADoUgqkATfVaqaqkqcPLdQdBQAAAOhSCqQB98F7dqSxfUtm5sxBAgAAAK5NgTTghoZK2q1Gnjl8IpeWVuqOAwAAAHQhBRJpTzRz9uJSnn35ZN1RAAAAgC6kQCKPPzyWLZuGnMYGAAAAXJMCidyxeTiPPzyWmbnjqaqq7jgAAABAl1EgkSRptxo5tvhmjsyfrTsKAAAA0GUUSCS5MgcpSaZnncYGAAAA/CQFEkmS9+wczYfu3WEOEgAAAPAOCiT+Rnuime+9diqL5y7VHQUAAADoIgok/ka71UhVJQfmPIUEAAAA/B8KJP7Gh+7Zmcb2LZmZMwcJAAAA+D8USPyNoaGSdquRZw6fyKWllbrjAAAAAF1CgcRPaE80c/biUr778mLdUQAAAIAuoUDiJzz+8Fi2bBrK9KxtbAAAAMAVCiR+wh2bh/P4w2OZmTueqqrqjgMAAAB0AQUS79BuNXJs8c0cmT9bdxQAAACgCyiQeIf2RDNJMjM7X3MSAAAAoBsokHiH9+wczQfv2ZEZc5AAAACAKJB4F+1WM9977VQWz12qOwoAAABQMwUS1zTVamSlSg7M2cYGAAAAg06BxDV96J6daWzfkv0KJAAAABh4CiSuaWioZHKikW8eXsilpZW64wAAAAA1UiDxrtqtZs5eXMp3X16sOwoAAABQIwUS7+qJh8eyZdNQpp3GBgAAAANNgcS7umPzcB5/eCwzc8dTVVXdcQAAAICaKJBY1eREI8cW38yL82frjgIAAADURIHEqtqtRpJketZpbAAAADCoFEis6u6dd+SD9+zIjDlIAAAAMLAUSFxXu9XM9147lcVzl+qOAgAAANRAgcR1tScaWamSpw/ZxgYAAACD6LoFUinl/lLKgVLKbCnlr0op/6Czflcp5RullCOd992d9VJK+e1SyoullO+XUj561Wd9vnP9kVLK569a/9lSyl907vntUkpZjy/Lrfmpe3dmfPuWzJiDBAAAAAPpRp5AWkryT6qqaiV5LMkXSykfSPKbSWaqqtqbZKbze5L8UpK9ndcXkvxOcqVwSvKlJB9P8rEkX3qrdOpc84Wr7vvs7X811srQUEl7opFvHl7IpaWVuuMAAAAAG+y6BVJVVT+qqup7nZ/PJJlNcm+SzyX5SueyryT5lc7Pn0vyB9UV30myq5Ryd5JfTPKNqqoWq6o6leQbST7b+duOqqr+tKqqKskfXPVZdIl2q5mzF5fy3ZcX644CAAAAbLCbmoFUSnkgyc8keTZJs6qqHyVXSqYkjc5l9yY5dtVtr3fWVlt//RrrdJEnHh7Llk1DmZlzGhsAAAAMmhsukEopdyb5H0n+YVVVP17t0musVbewfq0MXyilHCylHFxYWLheZNbQHZuH88mH9mRmdj5XHhQDAAAABsUNFUillJFcKY/+S1VV/7OzfLyz/Syd97cmLL+e5P6rbr8vyQ+vs37fNdbfoaqq362q6tGqqh4dHx+/keisoXarmdcWz+fF+bN1RwEAAAA20I2cwlaS/H6S2aqq/t1Vf/pqkrdOUvt8kj++av3XOqexPZbkjc4Wt68n+UwpZXdnePZnkny987czpZTHOv/Wr131WXSRduvKLsVpp7EBAADAQLmRJ5AeT/J3k0yWUl7ovH45yW8l+YVSypEkv9D5PUm+luRokheT/F6Sv58kVVUtJvmXSZ7rvP5FZy1J/l6S/9S556Uk/2sNvhtr7O6dd+SD9+zIfnOQAAAAYKBsut4FVVV9O9eeU5Qk7WtcXyX54rt81peTfPka6weTfOh6Wahfe6KR/+vAizl17lJ2b9tcdxwAAABgA9zUKWzQbjWzUiUHDtnGBgAAAINCgcRN+al7d2Z8+5bMmIMEAAAAA0OBxE0ZGippTzTyzOGFXFpaqTsOAAAAsAEUSNy0yYlGzlxcynOvLF7/YgAAAKDnKZC4aU/sHcvmTUOZnnUaGwAAAAwCBRI3bevmTXn8oT2ZmZ3PlUP3AAAAgH6mQOKWtFvNvLZ4Pi8tnK07CgAAALDOFEjcksmJRpJk2mlsAAAA0PcUSNySe3bdkQ/cvSMz5iABAABA31MgccumWo08/+qpnDp3qe4oAAAAwDpSIHHL2q1mVqrk6cO2sQEAAEA/UyBxy37q3p0Z377FHCQAAADocwokbtnQUMnkvkaeObSQS0srdccBAAAA1okCidvSbjVy5uJSnntlse4oAAAAwDpRIHFbntg7ls2bhjJjGxsAAAD0LQUSt2Xr5k355EN7MjN3PFVV1R0HAAAAWAcKJG5bu9XMqyfP56WFs3VHAQAAANaBAonb1p5oJInT2AAAAKBPKZC4bffsuiMfuHtHZmaP1x0FAAAAWAcKJNbEVKuR5189lVPnLtUdBQAAAFhjCiTWxGSrmZUqefqwbWwAAADQbxRIrImfvndnxrdvMQcJAAAA+pACiTUxNFQyua+RZw4t5NLSSt1xAAAAgDWkQGLNtFuNnLm4lIOvLNYdBQAAAFhDCiTWzBN7x7J505BtbAAAANBnFEisma2bN+WTD+3JzNzxVFVVdxwAAABgjSiQWFPtVjOvnjyflxbO1h0FAAAAWCMKJNZUe6KRJJmxjQ0AAAD6hgKJNXXPrjvSunuHAgkAAAD6iAKJNTfVauTgq4s5de5S3VEAAACANaBAYs21W82sVMnThz2FBAAAAP1AgcSa++l7d2bszi22sQEAAECfUCCx5oaGSiYnxvPNwwu5vLxSdxwAAADgNimQWBftVjNnLizluZcX644CAAAA3CYFEuviU3vHsnnTUKZtYwMAAICep0BiXWzdvCmffGhPZuaOp6qquuMAAAAAt0GBxLppTzTy6snzeWnhXN1RAAAAgNugQGLdTLaaSZKZ2eM1JwEAAABuhwKJdXPvrjvSuntHZsxBAgAAgJ6mQGJdTbUaOfjqYk6fv1R3FAAAAOAWKZBYV5MTjaxUydOHFuqOAgAAANwiBRLr6sP37crYnVsybQ4SAAAA9CwFEutqaKhkcmI83zy8kMvLK3XHAQAAAG6BAol11241c+bCUp57ebHuKAAAAMAtUCCx7p54eCybNw1lZs5pbAAAANCLFEisu21bNuUTD+7JzOzxVFVVdxwAAADgJimQ2BBTrUZeOXk+Ly2cqzsKAAAAcJMUSGyIyVYzSTLjNDYAAADoOQokNsS9u+5I6+4d5iABAABAD1IgsWHaE408/+qpnD5/qe4oAAAAwE1QILFh2q1GlleqPH1ooe4oAAAAwE1QILFhPnzfrozduSXT5iABAABAT1EgsWGGhkomJ8bzzcMLuby8UnccAAAA4AYpkNhQkxPNnLmwlOdeWaw7CgAAAHCDFEhsqE/tHcvm4aHMzDqNDQAAAHqFAokNtW3LpnzioT2ZmT2eqqrqjgMAAADcAAUSG26q1cgrJ8/n6IlzdUcBAAAAboACiQ331EQjSTLjNDYAAADoCQokNtx9u7dm4j3bM20OEgAAAPQEBRK1mGo18/yrp3L6/KW6owAAAADXoUCiFu1WI8srVb55eKHuKAAAAMB1KJCoxYfv25WxOzfbxgYAAAA9QIFELYaGSp7a18jTh+ZzeXml7jgAAADAKhRI1KbdaubMhaU898pi3VEAAACAVSiQqM2n9o5l8/BQ9tvGBgAAAF1NgURttm3ZlMce2pOZOQUSAAAAdDMFErWaajXy8olzeWnhbN1RAAAAgHehQKJWkxONJMnM7PGakwAAAADvRoFEre7bvTUT79meaXOQAAAAoGspkKhdu9XI86+eyunzl+qOAgAAAFyDAonatVvNLK9U+ebhhbqjAAAAANegQKJ2H7lvV8bu3GwbGwAAAHQpBRK1GxoqeWpfI08fms/l5ZW64wAAAABvo0CiK7RbzZy5sJSDr5yqOwoAAADwNgokusKn9o5l8/BQZmaP1x0FAAAAeBsFEl1h25ZNeeyhPZmZMwcJAAAAuo0Cia4x1Wrk5RPn8tLC2bqjAAAAAFdRINE1JicaSZL9TmMDAACArnLdAqmU8uVSynwp5S+vWvvnpZQflFJe6Lx++aq//bNSyoullEOllF+8av2znbUXSym/edX6+0spz5ZSjpRS/lspZfNafkF6x327t2biPdszbQ4SAAAAdJUbeQLpPyf57DXW/31VVR/pvL6WJKWUDyT51SQf7NzzH0spw6WU4ST/IckvJflAkr/TuTZJ/nXns/YmOZXk12/nC9Hb2q1GDr56Km+cv1x3FAAAAKDjugVSVVXPJFm8wc/7XJI/rKrqYlVVLyd5McnHOq8Xq6o6WlXVpSR/mORzpZSSZDLJf+/c/5Ukv3KT34E+0m41s7xS5enDtrEBAABAt7idGUi/UUr5fmeL2+7O2r1Jjl11zeudtXdb35PkdFVVS29bZ0B95L5d2bNtc2bMQQIAAICucasF0u8keSjJR5L8KMm/7ayXa1xb3cL6NZVSvlBKOVhKObiwsHBziekJQ0MlT0008vSh+VxeXqk7DgAAAJBbLJCqqjpeVdVyVVUrSX4vV7aoJVeeILr/qkvvS/LDVdZPJNlVStn0tvV3+3d/t6qqR6uqenR8fPxWotMDplqN/PjCUg6+cqruKAAAAEBusUAqpdx91a9/K8lbJ7R9NcmvllK2lFLen2Rvku8meS7J3s6Ja5tzZdD2V6uqqpIcSPK3O/d/Pskf30om+sen9o5n8/BQZpzGBgAAAF3hugVSKeW/JvnTJPtKKa+XUn49yb8ppfxFKeX7SZ5K8o+SpKqqv0ryR0n+d5I/SfLFzpNKS0l+I8nXk8wm+aPOtUnyT5P841LKi7kyE+n31/Qb0nO2bdmUxx7ak/1z5iABAABANyhXHgLqPY8++mh18ODBumOwTr7y/72SL331r7L/n/x8Hhy/s+44AAAAMBBKKc9XVfXo29dv5xQ2WDftViNJnMYGAAAAXUCBRFe6b/fWTLxne6bNQQIAAIDaKZDoWu1WIwdfPZU3zl+uOwoAAAAMNAUSXWtyopnllSpPH7aNDQAAAOqkQKJrfeT+XdmzbbM5SAAAAFAzBRJda3io5KmJRp4+NJ/Lyyt1xwEAAICBpUCiq021GvnxhaUcfOVU3VEAAABgYCmQ6GpP7B3P5uGh7J9zGhsAAADURYFEV7tzy6Z8/MG7zEECAACAGimQ6HpTrWaOnjiXowtn644CAAAAA0mBRNdrtxpJ4ikkAAAAqIkCia533+6tmXjP9syYgwQAAAC1UCDREyYnGnnulVN54/zluqMAAADAwFEg0RParWaWV6o8fdg2NgAAANhoCiR6wkfu35U92zabgwQAAAA1UCDRE4aHSp6aaOTpQ/NZWl6pOw4AAAAMFAUSPaM90ciPLyzl4Kun6o4CAAAAA0WBRM/41CPj2Tw8lJlZp7EBAADARlIg0TPu3LIpH3/wLnOQAAAAYIMpkOgpU61mjp44l6MLZ+uOAgAAAANDgURPmZxoJEn2z3kKCQAAADaKAomecv9dW7OvuT3T5iABAADAhlEg0XParUaee+VU3jh/ue4oAAAAMBAUSPScdquZ5ZUq3zyyUHcUAAAAGAgKJHrOR+7flbu2bc6MbWwAAACwIRRI9JzhoZKn9jXy9KGFLC2v1B0HAAAA+p4CiZ401WrkjTcv5+Crp+qOAgAAAH1PgURP+tQj4xkZLtk/N193FAAAAOh7CiR60p1bNuWxB/dk2hwkAAAAWHcKJHpWe6KRowvn8vKJc3VHAQAAgL6mQKJntVvNJHEaGwAAAKwzBRI96/67tmZfc7ttbAAAALDOFEj0tMlWI8+9cipvvHm57igAAADQtxRI9LSpViPLK1W+eXih7igAAADQtxRI9LSP3L87d23bbA4SAAAArCMFEj1teKjkqX2NPH1oIUvLK3XHAQAAgL6kQKLnTbUaeePNy3n+1VN1RwEAAIC+pECi5z2xdywjwyUzc/N1RwEAAIC+pECi520fHcljD+7JtDlIAAAAsC4USPSF9kQjRxfO5eUT5+qOAgAAAH1HgURfaLeaSeI0NgAAAFgHCiT6wv13bc0jzTszM2sOEgAAAKw1BRJ9o91q5rlXFvPGm5frjgIAAAB9RYFE35hqNbK0UuWbhxfqjgIAAAB9RYFE3/jI/btz17bN2W8OEgAAAKwpBRJ9Y3io5Ml94zlwaCFLyyt1xwEAAIC+oUCir0y1mnnjzct5/tVTdUcBAACAvqFAoq98au9YRoZLZuacxgYAAABrRYFEX9k+OpLHHtyTGXOQAAAAYM0okOg7kxONvLRwLq+cOFd3FAAAAOgLCiT6zlSrmSSZ9hQSAAAArAkFEn3n/ru25pHmnZmZNQcJAAAA1oICib7UbjXz3CuLeePNy3VHAQAAgJ6nQKIvtScaWVqp8szhhbqjAAAAQM9TINGXfua9u3PXts1OYwMAAIA1oECiLw0PlTy5bzwHDi1kaXml7jgAAADQ0xRI9K2pVjNvvHk5z796qu4oAAAA0NMUSPStT+0dy8hwyf45p7EBAADA7VAg0be2j47k4+/fk2lzkAAAAOC2KJDoa+1WIy8tnMsrJ87VHQUAALLynHkAAB2/SURBVAB6lgKJvjbVaiaJp5AAAADgNiiQ6Gv337U1jzTvNAcJAAAAboMCib43OdHMd19ezI8vXK47CgAAAPQkBRJ9b6rVyNJKlW8eWqg7CgAAAPQkBRJ972feuzt3bducGXOQAAAA4JYokOh7w0MlT+4bz9OHF7K0vFJ3HAAAAOg5CiQGQnuimdPnL+d7r52uOwoAAAD0HAUSA+HTj4xlZLjYxgYAAAC3QIHEQNg+OpKPv39PphVIAAAAcNMUSAyMdquRlxbO5ZUT5+qOAgAAAD1FgcTAaE80kyQzc/M1JwEAAIDeokBiYLx3z9bsbdxpDhIAAADcJAUSA6Xdaua7Ly/mxxcu1x0FAAAAeoYCiYEy1WpkaaXKM4cX6o4CAAAAPUOBxED5mffuzu6tI5mZNQcJAAAAbpQCiYEyPFTy1L5GDhyaz9LySt1xAAAAoCcokBg47VYzp89fzvdeO113FAAAAOgJCiQGzqcfGcumoZKZOaexAQAAwI1QIDFwto+O5OMP3mUOEgAAANyg6xZIpZQvl1LmSyl/edXaXaWUb5RSjnTed3fWSynlt0spL5ZSvl9K+ehV93y+c/2RUsrnr1r/2VLKX3Tu+e1SSlnrLwlv155o5sX5s3n15Lm6owAAAEDXu5EnkP5zks++be03k8xUVbU3yUzn9yT5pSR7O68vJPmd5ErhlORLST6e5GNJvvRW6dS55gtX3ff2fwvW3FSrmSSZ9hQSAAAAXNd1C6Sqqp5Jsvi25c8l+Urn568k+ZWr1v+guuI7SXaVUu5O8otJvlFV1WJVVaeSfCPJZzt/21FV1Z9WVVUl+YOrPgvWzXv3bM3exp2ZmTUHCQAAAK7nVmcgNauq+lGSdN4bnfV7kxy76rrXO2urrb9+jXVYd5OtRr778mJ+fOFy3VEAAACgq631EO1rzS+qbmH92h9eyhdKKQdLKQcXFhZuMSJcMdVqZmmlyjOH/bcEAAAAq7nVAul4Z/tZOu9vDZJ5Pcn9V113X5IfXmf9vmusX1NVVb9bVdWjVVU9Oj4+fovR4YqPvnd3dm8dcRobAAAAXMetFkhfTfLWSWqfT/LHV63/Wuc0tseSvNHZ4vb1JJ8ppezuDM/+TJKvd/52ppTyWOf0tV+76rNgXQ0PlTy1r5EDh+aztLxSdxwAAADoWtctkEop/zXJnybZV0p5vZTy60l+K8kvlFKOJPmFzu9J8rUkR5O8mOT3kvz9JKmqajHJv0zyXOf1LzprSfL3kvynzj0vJflfa/PV4PrarWZOn7+cPzt2uu4oAAAA0LU2Xe+Cqqr+zrv8qX2Na6skX3yXz/lyki9fY/1gkg9dLwesh089MpZNQyXTs8fzcw/cVXccAAAA6EprPUQbesqO0ZF8/MG7zEECAACAVSiQGHjtiWZenD+bV0+eqzsKAAAAdCUFEgOv3WokiaeQAAAA4F0okBh479uzLQ837szM3PG6owAAAEBXUiBBrjyF9OzRxfz4wuW6owAAAEDXUSBBkqlWM0srVZ45vFB3FAAAAOg6CiRI8tH37s6urSPZbw4SAAAAvIMCCZIMD5U8ta+RA4fms7xS1R0HAAAAuooCCTrarUZOnb+c7712qu4oAAAA0FUUSNDx6UfGs2moZHrWaWwAAABwNQUSdOwYHcnHH7zLHCQAAAB4GwUSXGVyopkj82fz2snzdUcBAACArqFAgqtMtRpJYhsbAAAAXEWBBFd5355tebhxZ2bmFEgAAADwFgUSvE271cizRxdz5sLluqMAAABAV1Agwdu0J5pZWqnyzOETdUcBAACArqBAgrf56Ht3ZdfWkcyYgwQAAABJFEjwDpuGh/LUvkYOHJrP8kpVdxwAAAConQIJrqHdauTU+cv53mun6o4CAAAAtVMgwTV8+pHxbBoqmZmdrzsKAAAA1E6BBNewY3QkH3v/XeYgAQAAQBRI8K7arWaOzJ/NayfP1x0FAAAAaqVAgncx1WokSaY9hQQAAMCAUyDBu3jfnm15uHFn9s+ZgwQAAMBgUyDBKtoTjTz78smcuXC57igAAABQGwUSrKLdaubycpVnDp+oOwoAAADURoEEq/joe3dl19YRp7EBAAAw0BRIsIpNw0N5al8jBw7NZ3mlqjsOAAAA1EKBBNcxOdHIqfOX82evnao7CgAAANRCgQTX8fP7xrNpqGR61mlsAAAADCYFElzHjtGRfOz9d5mDBAAAwMBSIMENaLeaOTJ/Nq+dPF93FAAAANhwCiS4Ae2JRpJkZs5TSAAAAAweBRLcgAfGtuWh8W2ZMQcJAACAAaRAghs01Wrm2ZdP5syFy3VHAQAAgA2lQIIb1G41c3m5yreOnKg7CgAAAGwoBRLcoI++d1d23jGSaaexAQAAMGAUSHCDNg0P5al943n60EKWV6q64wAAAMCGUSDBTWi3mlk8dyl/9tqpuqMAAADAhlEgwU349CPj2TRUMjPnNDYAAAAGhwIJbsLOO0bycw/clRlzkAAAABggCiS4Se1WI4ePn82xxfN1RwEAAIANoUCCmzTVaiaJ09gAAAAYGAokuEkPjG3LQ+PbMjNrDhIAAACDQYEEt6DdaubZl0/mzIXLdUcBAACAdadAglvQnmjk8nKVbx05UXcUAAAAWHcKJLgFP/u+3dl5x4g5SAAAAAwEBRLcgk3DQ3lq33iePrSQ5ZWq7jgAAACwrhRIcIvarWYWz13KC8dO1R0FAAAA1pUCCW7Rpx8Zz6ahkmmnsQEAANDnFEhwi3beMZKfe+CuzJiDBAAAQJ9TIMFtaLcaOXz8bI4tnq87CgAAAKwbBRLchqlWM0k8hQQAAEBfUyDBbXhgbFseHN+WmTlzkAAAAOhfCiS4TVOtZr5z9GTOXLhcdxQAAABYFwokuE3tiUYuL1f51pETdUcBAACAdaFAgtv0s+/bnZ13jGRm1jY2AAAA+pMCCW7TpuGhPLlvPAcOzWd5pao7DgAAAKw5BRKsgXarmcVzl/LCsVN1RwEAAIA1p0CCNfDzj4xn01DJtG1sAAAA9CEFEqyBnXeM5OceuCv7FUgAAAD0IQUSrJF2q5FDx8/k2OL5uqMAAADAmlIgwRppt5pJkpnZ4zUnAQAAgLWlQII18v6xbXlwfFtm5mxjAwAAoL8okGANTbWa+c7Rkzl7canuKAAAALBmFEiwhiYnGrm8XOVbhxfqjgIAAABrRoEEa+jR9+3OzjtGMu00NgAAAPqIAgnW0KbhoTy5bzwHDs1neaWqOw4AAACsCQUSrLF2q5nFc5fywrFTdUcBAACANaFAgjX283vHMzxUMmMbGwAAAH1CgQRrbOfWkfzcA7sVSAAAAPQNBRKsg6lWM4eOn8mxxfN1RwEAAIDbpkCCddBuNZMkM7PHa04CAAAAt0+BBOvg/WPb8uD4tszM2cYGAABA71MgwTppTzTy7NHFnL24VHcUAAAAuC0KJFgn7VYzl5ZX8q3DC3VHAQAAgNuiQIJ18uj7dmfnHSOZdhobAAAAPU6BBOtk0/BQntw3nqcPzWd5pao7DgAAANyy2yqQSimvlFL+opTyQinlYGftrlLKN0opRzrvuzvrpZTy26WUF0sp3y+lfPSqz/l85/ojpZTP395Xgu4xOdHIyXOX8sKx03VHAQAAgFu2Fk8gPVVV1Ueqqnq08/tvJpmpqmpvkpnO70nyS0n2dl5fSPI7yZXCKcmXknw8yceSfOmt0gl63ZOPNDI8VDIze7zuKAAAAHDL1mML2+eSfKXz81eS/MpV639QXfGdJLtKKXcn+cUk36iqarGqqlNJvpHks+uQCzbczq0j+bkHdmfGHCQAAAB62O0WSFWS/7eU8nwp5QudtWZVVT9Kks57o7N+b5JjV937emft3dahL0y1mjl0/EyOLZ6vOwoAAADcktstkB6vquqjubI97YullE+vcm25xlq1yvo7P6CUL5RSDpZSDi4sOBqd3jA5caVD3T/nKSQAAAB6020VSFVV/bDzPp/k/86VGUbHO1vT0nl/6/+aX09y/1W335fkh6usX+vf+92qqh6tqurR8fHx24kOG+bB8Tvz4Ni2TJuDBAAAQI+65QKplLKtlLL9rZ+TfCbJXyb5apK3TlL7fJI/7vz81SS/1jmN7bEkb3S2uH09yWdKKbs7w7M/01mDvtFuNfLs0cWcvbhUdxQAAAC4abfzBFIzybdLKX+e5LtJ/p+qqv4kyW8l+YVSypEkv9D5PUm+luRokheT/F6Sv58kVVUtJvmXSZ7rvP5FZw36RrvVzKXllXz7iK2XAAAA9J5Nt3pjVVVHk3z4Gusnk7SvsV4l+eK7fNaXk3z5VrNAt/vZ9+3OjtFNmZ6dz2c/dHfdcQAAAOCm3O4QbeAGjAwP5cl9jRyYm8/yyjVnxAMAAEDXUiDBBmm3Gjl57lJeOHa67igAAABwUxRIsEGefKSR4aGS/XNOYwMAAKC3KJBgg+zcOpJH37c7M7PzdUcBAACAm6JAgg001Wpm7q/P5PVT5+uOAgAAADdMgQQbqN1qJImnkAAAAOgpCiTYQA+O35kHx7ZletYcJAAAAHqHAgk22OREI88eXczZi0t1RwEAAIAbokCCDdZuNXNpeSXfPrJQdxQAAAC4IQok2GCPPrA7O0Y3ZdocJAAAAHqEAgk22MjwUJ7c18iBufksr1R1xwEAAIDrUiBBDdqtRk6eu5Q/f/103VEAAADguhRIUIMnH2lkeKhkxmlsAAAA9AAFEtRg59aRPPq+3ZkxBwkAAIAeoECCmky1mpn76zN5/dT5uqMAAADAqhRIUJN2q5Ek2T/nKSQAAAC6mwIJavLg+J15/9i2TNvGBgAAQJdTIEGN2hONfOelkzl7canuKAAAAPCuFEhQo3armUvLK/n2kYW6owAAAMC7UiBBjR59YHd2jG5yGhsAAABdTYEENRoZHsrP72vkwKH5rKxUdccBAACAa1IgQc2mWo2cOHspL7x+uu4oAAAAcE0KJKjZk480MjxUMjN7vO4oAAAAcE0KJKjZzq0jefR9u81BAgAAoGspkKALtFuNzP31mbx+6nzdUQAAAOAdFEjQBdqtZpJk/5ynkAAAAOg+CiToAg+N35n3j23LtG1sAAAAdCEFEnSJ9kQj33npZM5dXKo7CgAAAPwEBRJ0iclWI5eWV/KtIyfqjgIAAAA/QYH0/7d3bzF2XfUZwL//3GzPxLGdYBtIyKURSYz6wCWBtkgkikMLLQIeqdQ+VeKlVNA+9PLIS9WHqmofqkoI6EVF0BaohNqol8SpStS0BFJCAJtCWi5OAo6Dx4kvyYw9qw9zxplxMgePx+N9tuf3kyyfOcd7n+8cL9vj76y1NoyIO2+6Jtu3TuSBgz/qOgoAAACsoECCETE5Ppa7b9uTB791JAsLres4AAAAcI4CCUbIvfv25OiJuXz18GzXUQAAAOAcBRKMkLtu3Z3xscoBV2MDAABghCiQYITsnJ7KW27clfvtgwQAAMAIUSDBiLl3354c+uHzOXzsVNdRAAAAIIkCCUbO/n17kyQHDlnGBgAAwGhQIMGIuWX3Vbn5VTN5wD5IAAAAjAgFEoyge27fk4efeDYnXzzTdRQAAABQIMEo2r9vT+bOLuSL3z7adRQAAABQIMEouvOma7J960QecDU2AAAARoACCUbQ5PhY7r5tTx781pEsLLSu4wAAALDJKZBgRO2/fU+OnpjLY4dnu44CAADAJqdAghF19227Mz5WrsYGAABA5xRIMKJ2Tk/lLTfuyv32QQIAAKBjCiQYYffu25NDP3w+T86e7joKAAAAm5gCCUbYPbfvTZIcMAsJAACADimQYITdsnsmN107nfvtgwQAAECHFEgwwqoq+/ftzcNPPJuTL57pOg4AAACblAIJRtz+fXsyd3YhD33naNdRAAAA2KQUSDDi7rzpmmzfOpEH7IMEAABARxRIMOImx8dy1627c+DQM1lYaF3HAQAAYBNSIEEP3Ltvb46eeDGPHZ7tOgoAAACbkAIJeuDu23ZnrJIHXI0NAACADiiQoAd2Tk/ljhuvyQOHFEgAAABcfgok6In9+/bk4NPP5cnZ011HAQAAYJNRIEFP7N+3N0lywNXYAAAAuMwUSNATt+yeyU3XTud++yABAABwmSmQoCeqKvfcvjcPP/FsTr54pus4AAAAbCIKJOiRe/ftydzZhTz0naNdRwEAAGATUSBBj9x58zXZvnUiD9gHCQAAgMtIgQQ9Mjk+lrtu3Z0Dh57JwkLrOg4AAACbhAIJeubefXtz9MSL+dqTx7uOAgAAwCahQIKeuevW3RmrWMYGAADAZaNAgp7ZNTOVO268JvcfPNJ1FAAAADYJBRL00P59e3Lw6efy5OzprqMAAACwCSiQoIf279ubJDlwyCwkAAAANp4CCXrolt0zufHaafsgAQAAcFkokKCHqir7b9+b/3ji2ZyaO9N1HAAAAK5wCiToqXv37cncmYV88dtHu44CAADAFU6BBD11583XZPuWiRxwNTYAAAA22ETXAYCLMzk+lnfctjv3H/xR/uaR72fHtqnsmp7Mzump7JyezM7pyWyZGO86JgAAAFcABRL02PvfeF3ue/zp/M7nHn/Fx7dNjmfX9GR2TC+VS5PLiqZB2bRt8efFXzeZndumMjVhciIAAAAvUSBBj73zDXvzjY/+Qo6dms/sqbnMnppf/HF66fbc4LHF2//zoxPnbp9ZaKued2ZqfMVMpqWiade5+5aKp2UznrZNZmJc8QQAAHAlUiBBz01PTWR6aiLX7dx2wce01nJy7myOnZzL8dPzObZUPp2ez+zJucwO7js+uO/g08+du312SPG0fctEds4szmJaWTwtzoLauW0yu2YmVyy327FtMuNjdSneCgAAADaIAgk2oarKVVsmctWWibxuDcctLLScmDuT2ZMvzXI6dmpQQp1cOfNp9vR8Dh87ndnB40N6p1y9dWLZMrpBubRtcugsqKu3TmZM8QQAAHBZKJCACzY2Vrl662J5c0OmL/i4hYWW5184szjT6fTy5XaLS+zOnwX1vWdPZvbUfJ57YT5tleKpKtmxbWXRtGswo2np9vnF047pyWzfMqF4AgAAWCMFErDhxsYqOwabdK/F2YWW55bKpdPzOX5qWdF0ammp3eLtH5+cyxPPLO7x9PwLZ1Y95/hYLSuelu/htFRCLZ8F9dIMqKu2TKRK8QQAAGxOCiRgZI2PVXbNTGXXzNSajjtzdmEwq2k+x88ttVs28+n0SxuOH3n+hXzrh8/n+On5nHhx9eJpYqwGV7F7aXbTy65o9wqzoKanxhVPAABA741MgVRV70ryJ0nGk3y8tfYHHUcCempifCzXXrUl1161ZU3HzZ1ZLJ6On37p6nVLm4mfPwvqqdkX8s2nnsvs6fmcmju76jmnxseyY7Cn09IyusXNxKdWlFHnL8XbOjmmeAIAAEbGSBRIVTWe5E+TvDPJ4SSPVNUXWmvf7DYZsJlMTYxl9/Yt2b19bcXTi2fOnrtK3bGTK/d5WjkLai4/+PGpPD6YBfXC/MLQLEvL6HYMltbt3DZ17ip3SzOfdmybyq5lV77bOjm+3rcBAADgZUaiQEry1iTfaa39b5JU1WeSvC+JAgkYeVsmxrPn6vHsuXrrmo57Yf7suSV1x06unPm0fLndsVPz+e7RUzl2ajazp+Yzd3b14mnr5NjQzcRXbCw+s/jz9q2TOX+y09LXlTrv66XH67yvV94PAABcWUalQLouyQ+WfX04yds6ygJwWWydHM+rd4zn1TsuvHhqreX0UvG0YjPxxcLp+HmzoL5z5MS5WVDzZ1e5pN0Gu+DyKSt/4WqPr3a+5fe9/BxrK8Ky6nO9cpbV8qw4Zo2l3Kqv4ZXO/Qp5hr2+n5Qlq74vF/LeXbjKRR10OQ65qNez+FxrP/Ci3rvL9H5fzk74UhfQGxF9I96PS33KjSjy+/JebkzSn/CMHXxu0sVHNd28zs3x+3kp9T1/0s3v+6Wy7zXb86F7Xt91jA01KgXSK42Sl/1Pp6o+mOSDSXLDDTdsdCaAkVNVmZ6ayPTURF67c9sFH9day6m5s8uuYvfS7KYTg6vWtcFfu629/Njl97dz9+cVj1tx+NKxF3jM+Y/nZY8PP+6CXscas+T8x8/PuJ7XserxKx/PsOe82NexxizLrfaca3FRx7z8W4MNep6LO24x3toOvJyvae3PcxHZLuJ5Fp/rIg9c7XyX9nSDk176s17qM25AxIsaoz/xnBuRs4PPSLr4WOZi/lz2UTe/n/1+b6+EodH3l3DVllGpVzbOqLzCw0let+zr65M8df4vaq19LMnHkuSOO+7o+/gCuGyqKjNbJjKzZSLX7+o6DQAA0DdjXQcYeCTJ66vq5qqaSvKBJF/oOBMAAAAAGZEZSK21M1X1oST/nGQ8ySdba9/oOBYAAAAAGZECKUlaa/clua/rHAAAAACsNCpL2AAAAAAYUQokAAAAAIZSIAEAAAAwlAIJAAAAgKEUSAAAAAAMpUACAAAAYCgFEgAAAABDKZAAAAAAGEqBBAAAAMBQCiQAAAAAhlIgAQAAADCUAgkAAACAoRRIAAAAAAylQAIAAABgKAUSAAAAAEMpkAAAAAAYSoEEAAAAwFAKJAAAAACGUiABAAAAMJQCCQAAAIChqrXWdYaLUlXPJPle1zkugVclOdp1CHrNGGK9jCHWyxhivYwh1ssYYr2MIdbrShpDN7bWdp9/Z28LpCtFVX25tXZH1znoL2OI9TKGWC9jiPUyhlgvY4j1MoZYr80whixhAwAAAGAoBRIAAAAAQymQuvexrgPQe8YQ62UMsV7GEOtlDLFexhDrZQyxXlf8GLIHEgAAAABDmYEEAAAAwFAKpI5U1Ser6khVfb3rLPRTVb2uqh6sqoNV9Y2q+nDXmeiXqtpaVV+qqscGY+ijXWein6pqvKr+u6r+oess9E9VfbeqHq+qr1bVl7vOQ/9U1c6q+mxVHRp8X/SzXWeiP6rqtsHfP0s/nquqj3Sdi/6oqt8cfC/99ar6dFVt7TrTRrGErSNV9Y4kJ5L8VWvtp7vOQ/9U1WuSvKa19mhVbU/ylSTvb619s+No9ERVVZKZ1tqJqppM8lCSD7fW/rPjaPRMVf1WkjuSXN1ae0/XeeiXqvpukjtaa0e7zkI/VdVfJvlia+3jVTWVZLq1Ntt1LvqnqsaTPJnkba2173Wdh9FXVddl8XvoN7TWTlfV3ya5r7X2F90m2xhmIHWktfbvSX7cdQ76q7X2dGvt0cHt55McTHJdt6nok7boxODLycEPnyqwJlV1fZJfSvLxrrMAm09VXZ3kHUk+kSSttTnlEeuwP8kTyiPWaCLJtqqaSDKd5KmO82wYBRJcAarqpiRvSvJf3SahbwZLj76a5EiSf22tGUOs1R8n+e0kC10Hobdakn+pqq9U1Qe7DkPv/FSSZ5L8+WAp7ceraqbrUPTWB5J8uusQ9Edr7ckkf5jk+0meTnK8tfYv3abaOAok6LmquirJ55J8pLX2XNd56JfW2tnW2huTXJ/krVVlSS0XrKrek+RIa+0rXWeh197eWntzkncn+fXBMn+4UBNJ3pzkz1prb0pyMsnvdhuJPhosf3xvkr/rOgv9UVW7krwvyc1JXptkpqp+pdtUG0eBBD022Lfmc0k+1Vr7fNd56K/BdP9/S/KujqPQL29P8t7BHjafSXJPVf11t5Hom9baU4OfjyT5+yRv7TYRPXM4yeFlM2g/m8VCCdbq3Ukeba39qOsg9Mq9Sf6vtfZMa20+yeeT/FzHmTaMAgl6arAB8ieSHGyt/VHXeeifqtpdVTsHt7dl8R/AQ92mok9aa7/XWru+tXZTFqf9H2itXbGfunHpVdXM4EIQGSw7+vkkrlDLBWut/TDJD6rqtsFd+5O4oAgX45dj+Rpr9/0kP1NV04P/n+3P4t60VyQFUkeq6tNJHk5yW1Udrqpf6zoTvfP2JL+axU/8ly47+otdh6JXXpPkwar6WpJHsrgHksuwA5fT3iQPVdVjSb6U5B9ba//UcSb65zeSfGrw79kbk/x+x3nomaqaTvLOLM4egQs2mP342SSPJnk8ix3LxzoNtYGqNRfcAQAAAGB1ZiABAAAAMJQCCQAAAIChFEgAAAAADKVAAgAAAGAoBRIAAAAAQymQAAAAABhKgQQAAADAUAokAAAAAIb6f/K/KyIBe9/6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.plot(range(1, 9), running_loss[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from this example, the cost function slowly descents each time we pass the dataset through the network and update the weights based on its errors. The Network continues to improve and produce more accurate predictions the more times it 'sees' the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
